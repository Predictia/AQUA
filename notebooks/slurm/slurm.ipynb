{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of slurm job creation and submission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "1. [Importing the Job Module](#1-importing-the-job-module)\n",
    "\n",
    "2. [Creating the Slurm Job](#2-creating-the-slurm-job)\n",
    "\n",
    "    2.1 [Submitting Jobs to SLURM Queues on Different Machines](#21-submitting-jobs-to-slurm-queues-on-different-machines)\n",
    "    \n",
    "    2.2 [Creating the Job with Manual Selection of Cores, Memory, and Walltime](#22-creating-the-job-with-manual-selection-of-cores-memory-and-walltime)\n",
    "\n",
    "    2.3 [Creating the Job with Exclusive Node Access](#23-creating-the-job-with-exclusive-node-access)\n",
    "\n",
    "    2.4 [Creating the Job with Maximum Available Resources per Node](#24-creating-the-job-with-maximum-available-resources-per-node)\n",
    "\n",
    "    2.5 [Redirecting the SLURM output to /any/path/you/want](#25-redirecting-the-slurm-output-to-anypathyouwant)\n",
    "    \n",
    "3. [Canceling the Slurm Job](#3-canceling-the-slurm-job)\n",
    "\n",
    "    3.1 [Canceling All Jobs for a User](#31-canceling-all-jobs-for-a-user)\n",
    "\n",
    "    3.2 [Canceling a Specific Job](#32-canceling-a-specific-job)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the Job Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slurm module contains several functions that allow users to create and manage SLURM jobs. These functions include:\n",
    "\n",
    " - `squeue`: Query and display information about jobs in the SLURM queue.\n",
    " - `job`: Submit a job to the SLURM queue with various configurable parameters.\n",
    " - `max_resources_per_node`: Retrieve the maximum available resources per node for a specified machine and queue.\n",
    " - `scancel`: Cancel one or more jobs in the SLURM queue.\n",
    "\n",
    "These functions provide a comprehensive toolkit for operating SLURM jobs efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aqua.slurm import slurm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Slurm Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Submitting Jobs to SLURM Queues on Different Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job() function is designed to submit jobs to a SLURM queue on various machines, utilizing configurations that are predefined in a YAML configuration file (`.aqua/aqua/slurm/config-slurm.yml`). This approach allows users to seamlessly initialize and manage a SLURM cluster directly within the notebook using `dask_jobqueue.SLURMCluster`.\n",
    "\n",
    "Users must specify the machine name if they want to use the default setup provided in the YAML configuration file. When a machine name is provided, the job() function automatically loads the corresponding configuration from the YAML file, which includes parameters such as memory, core allocation, walltime, and more.\n",
    "\n",
    "⚠️ Warning: Currently, the pip installation does not copy the YAML configuration file to a user-accessible directory. This functionality will be updated in the future to ensure easier modification of configurations by users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Warning: There is no default machine name. Users need to provide a machine name to use the default setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;255;165;0m2024-05-17 17:37:15 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p small\n",
      "#SBATCH -A project_465000454\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=10G\n",
      "#SBATCH -t 02:30:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "\n",
      "/users/nazarova/mambaforge/aqua/bin/python3.11 -m distributed.cli.dask_worker tcp://193.167.209.163:44587 --name dummy-name --nthreads 1 --memory-limit 9.31GiB --nanny --death-timeout 60\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.job(machine_name='lumi')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the status of created Job in the queue using the function `squeue()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n",
      "7149698    1     1     PD         dask-worker          0:00       N/A                  (null)               small                10G                 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cancel all the jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, when submitting a job on `Lumi`, the job has the following attributes:\n",
    "\n",
    "- `exclusive=False`: The job will not request exclusive access to the node. Set to True to request exclusive access.\n",
    "- `max_resources=False`: The job will not request the maximum resources available on the node. Set to True to request maximum resources.\n",
    "- `cores=1`: The number of cores per socket allocated for the job.\n",
    "- `memory=\"10 GB\"`: The real memory required per node.\n",
    "- `walltime=\"02:30:00\"`: The duration for which the nodes remain allocated.\n",
    "- `jobs=1`: The factor of assignment scaling across multiple nodes.\n",
    "- `path_to_output=\".\"`: The path where log, error, and output files are stored.\n",
    "- `account=\"project_465000454\"`: The project account under which the job runs.\n",
    "- `queue=\"small\"`: The name of the queue to which the job is submitted.\n",
    "\n",
    "The YAML file also contains the default setup for the `Levante` and `Mafalda` machines, ensuring that job submissions on these machines are configured with appropriate settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating the Job with Manual Selection of Cores, Memory, and Walltime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For greater flexibility, users can manually specify the number of cores, memory, and walltime when creating a job. This allows customization beyond the default configurations, ensuring the job meets specific resource requirements. By adjusting these parameters, users can optimize resource usage and job performance according to their unique needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;255;165;0m2024-05-17 17:37:15 :: slurm :: WARNING  -> Machine name not specified. Please manually specify all configurations such as queue, cores, memory, etc.\u001b[0m\n",
      "/LUMI_TYKKY_eiW6rEt/miniconda/envs/env1/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 37883 instead\n",
      "  warnings.warn(\n",
      "\u001b[38;2;255;165;0m2024-05-17 17:37:15 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p interactive\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=8\n",
      "#SBATCH --mem=47G\n",
      "#SBATCH -t 00:30:00\n",
      "#SBATCH --error=None/slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=None/slurm/output/dask-worker-%j.out\n",
      "\n",
      "/users/nazarova/mambaforge/aqua/bin/python3.11 -m distributed.cli.dask_worker tcp://193.167.209.163:40101 --name dummy-name --nthreads 2 --memory-limit 11.64GiB --nworkers 4 --nanny --death-timeout 60\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.job(cores=8, memory=\"50 GB\", queue=\"interactive\", walltime='00:30:00', jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n",
      "7149810    8     1     PD         dask-worker          0:00       N/A                  (null)               interactive          47G                 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;64;184;50m2024-05-17 17:37:15 :: slurm :: INFO     -> Cancelling all user jobs in the queue\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.scancel(loglevel=\"INFO\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Creating the Job with Exclusive Node Access"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job() function includes an argument exclusive, which is set to False by default. By setting this argument to True, the job will request exclusive access to the node. This means no other jobs will be scheduled on the same node, ensuring that the entire node's resources are dedicated to your job. This option is useful for tasks that require all of a node's resources or need to avoid resource contention with other jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/LUMI_TYKKY_eiW6rEt/miniconda/envs/env1/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 42125 instead\n",
      "  warnings.warn(\n",
      "\u001b[38;2;255;165;0m2024-05-17 17:37:15 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p small\n",
      "#SBATCH -A project_465000454\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=10G\n",
      "#SBATCH -t 02:30:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "#SBATCH --get-user-env\n",
      "#SBATCH --exclusive\n",
      "\n",
      "/users/nazarova/mambaforge/aqua/bin/python3.11 -m distributed.cli.dask_worker tcp://193.167.209.163:42349 --name dummy-name --nthreads 1 --memory-limit 9.31GiB --nanny --death-timeout 60\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.job(exclusive=True, machine_name='lumi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;64;184;50m2024-05-17 17:37:16 :: slurm :: INFO     -> Cancelling the job with ID: 7148312\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.scancel(Job_ID=7148312, loglevel=\"INFO\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Warning: The `exclusive` argument DOES NOT automatically provide us the maximum available memory, number of cores, and walltime!\n",
    "It only provide you the exclusive usage of the node, meaning that no other job can run at the same time on the same node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;255;165;0m2024-05-17 17:37:16 :: slurm :: WARNING  -> Machine name not specified. Please manually specify all configurations such as queue, cores, memory, etc.\u001b[0m\n",
      "/LUMI_TYKKY_eiW6rEt/miniconda/envs/env1/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 43051 instead\n",
      "  warnings.warn(\n",
      "\u001b[38;2;255;165;0m2024-05-17 17:37:16 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p interactive\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=32\n",
      "#SBATCH --mem=187G\n",
      "#SBATCH -t 8:00:00\n",
      "#SBATCH --error=None/slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=None/slurm/output/dask-worker-%j.out\n",
      "#SBATCH --get-user-env\n",
      "#SBATCH --exclusive\n",
      "\n",
      "/users/nazarova/mambaforge/aqua/bin/python3.11 -m distributed.cli.dask_worker tcp://193.167.209.163:41007 --name dummy-name --nthreads 4 --memory-limit 23.28GiB --nworkers 8 --nanny --death-timeout 60\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.job(exclusive=True, cores=32, memory=\"200 GB\", queue = \"interactive\", walltime='8:00:00', jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n",
      "7149811    1     1     PD         dask-worker          0:00       N/A                  (null)               small                10G                 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Creating the Job with Maximum Available Resources per Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the argument `max_resources_per_node=True`, the function `max_resources_per_node()` automatically extracts the following information about the resources in any queue:\n",
    "\n",
    " - Size of memory per node in Gigabytes: `max_memory`, \n",
    "\n",
    " - Maximum time for any job in the format \"days-hours:minutes:seconds: `max_walltime`\n",
    "\n",
    " - Number of CPUs per node: `max_cpus`, \n",
    "\n",
    " - Number of sockets per node: `max_sockets`\n",
    "\n",
    " - Number of cores per socket: `max_cores`, \n",
    " \n",
    " - Number of threads per core: `max_threads`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following call reads the maximum available resources for the queue specified in the YAML configuration file for the machine `Lumi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('99.19921875 GB', '3-00:00:00', '128', '2', '64', '2')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.max_resources_per_node(machine_name='lumi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('99.19921875 GB', '3-00:00:00', '128', '2', '64', '2')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.max_resources_per_node(queue='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function can be also used alone to have info about a specific queue/partition.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('224.0 GB', '8:00:00', '128', '2', '64', '2')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.max_resources_per_node(queue='interactive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job() function includes an argument max_resources_per_node, which is set to False by default. By setting this argument to True, the job will utilize the maximum available resources per node, as allowed by the selected queue or partition. This ensures that the job leverages the full capacity of the node, maximizing the number of cores, memory, and walltime allocated to the job. This option is ideal for tasks that require extensive computational resources and need to maximize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/LUMI_TYKKY_eiW6rEt/miniconda/envs/env1/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 44451 instead\n",
      "  warnings.warn(\n",
      "\u001b[38;2;255;165;0m2024-05-17 17:37:18 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p small\n",
      "#SBATCH -A project_465000454\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=128\n",
      "#SBATCH --mem=93G\n",
      "#SBATCH -t 3-00:00:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "\n",
      "/users/nazarova/mambaforge/aqua/bin/python3.11 -m distributed.cli.dask_worker tcp://193.167.209.163:36743 --name dummy-name --nthreads 8 --memory-limit 5.77GiB --nworkers 16 --nanny --death-timeout 60\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.job(max_resources=True, machine_name='lumi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Redirecting the SLURM output to `/any/path/you/want`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slurm Job writes by default\n",
    "    - the errors into `./slurm/logs` directory \n",
    "    - the output into `./slurm/output/` directory\n",
    "\n",
    "If folders `/slurm`,  `/slurm/output`,  `/slurm/logs` do not exist, the function will create them automatically. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can specify a custom path to redirect the SLURM outputs with the `path_to_output` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.job(path_to_output='/any/path/you/want', machine_name='lumi')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Canceling the Slurm Job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Canceling All Jobs for a User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Canceling a Specific Job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the Job_ID, you can cancel your Job in the queue. For exaple, you can find your Job_ID using the function `squeue().` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_ID = 4929434\n",
    "slurm.scancel(Job_ID)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the status of Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tropical-rainfall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
